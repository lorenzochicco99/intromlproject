{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 09:44:11.246958: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-24 09:44:11.941670: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from utils.downloader import *\n",
    "from utils.dataset_reader import *\n",
    "from utils.extractors import *\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Aggiungere nella pipeline di preparazione dei dati di pytorch la possibilità di fare da augmentation.\n",
    "- I logs sono salvati in datasets al momento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook usage\n",
    "This notebook is used to streamline as much as possible the data setup for the project. We have trained models on data downloaded from the internet \n",
    "as well as present in the Torch and timm libraries. \n",
    "\n",
    "This notebook will help you set up the data in the correct directory structure.\n",
    "It will be used to download the data, extract it, and set it up in the correct directory structure.\n",
    "If you want to work using datasets preloaded in Torch or timm, you can skip this notebook.\n",
    "\n",
    "Everything that needs to be edited is marked with \"@edit\". It is procedural, therefore do not skip steps and go one cell at the time. \n",
    "\n",
    "At the moment less common cases such as image folder + .mat file or .csv file are not yet implemented in this notebook (csv function exists) or not implemented at all (.mat function doesn't exist at all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually edit the three following lines, save the dataset wherever you wish, but the \"datasets\" folder is recommended\n",
    "# make sure to use the absolute path, not relative\n",
    "# @edit\n",
    "dataset_url = \"https://www.kaggle.com/datasets/prasanshasatpathy/soil-types\"\n",
    "# @edit\n",
    "folder_target = \"/home/lorenzo/Desktop/ml/intromlproject_su cui lavorare/datasets\"\n",
    "download_dataset_kaggle(dataset_url, folder_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download dataset from a url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @edit\n",
    "dataset_url = \"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\"\n",
    "folder_target = '\"/home/disi/ml/datasets/\"'\n",
    "i = download_dataset(dataset_url, folder_target)\n",
    "# Manually move it in the datasets folder if you do not need to unzip it, otherwhise extracting it with the following function\n",
    "# will do it for you\n",
    "# @edit\n",
    "extract_tgz(f\"{dataset_url}/datasets.tar.gz\", folder_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment the dataset (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @edit\n",
    "# make sure to add a / in front of the folder name \n",
    "dataset_folder = folder_target + \"/Soil types\"\n",
    "image_generator =  setup_data_generator(rotation_range=40,\n",
    "                                        width_shift_range=0.2, \n",
    "                                        height_shift_range=0.2,\n",
    "                                        shear_range=0.2, \n",
    "                                        zoom_range=0.2, \n",
    "                                        horizontal_flip=True, \n",
    "                                        fill_mode='nearest')\n",
    "\n",
    "create_train_val_test_folders(dataset_folder, train_size=0.7, val_size=0.15, test_size=0.15)\n",
    "cleanup(dataset_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path  = dataset_folder + \"/train\"\n",
    "train_generator = load_data_from_directory(directory_path, target_size=(300,300), batch_size=32)\n",
    "n_of_batches = 50\n",
    "\n",
    "#TODO write how the number of batches infuences the actual amount of images generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (images, labels) in enumerate(train_generator):\n",
    "    if i >= n_of_batches:  # Stop after saving images from 50 batches\n",
    "        break\n",
    "    save_augmented_images(images, labels, directory_path, train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test-Val-Split\n",
    "If you augmented the images you are good to go, the dataset is ready to be used in the main function. Otherwhise edit and execute the following block of code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @edit\n",
    "# make sure to add a / in front of the folder name \n",
    "dataset_folder = folder_target + \"/jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_train_val_test_folders(dataset_folder, train_size=0.7, val_size=0.15, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets from Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FGVCAircraft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/archives/fgvc-aircraft-2013b.tar.gz to /home/disi/ml/datasets/fgvc-aircraft-2013b.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2753340328/2753340328 [00:33<00:00, 81301460.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/disi/ml/datasets/fgvc-aircraft-2013b.tar.gz to /home/disi/ml/datasets/\n"
     ]
    }
   ],
   "source": [
    "# @edit\n",
    "root = \"/home/disi/ml/datasets/\"\n",
    "dataset = torchvision.datasets.FGVCAircraft(root=root, download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @edit\n",
    "dataset_path = f'{root}/fgvc-aircraft-2013b/data/images'\n",
    "new_dataset_path = f'{root}'\n",
    "train_txt_path = f'{root}/fgvc-aircraft-2013b/data/images_family_train.txt'\n",
    "val_txt_path = f'{root}/fgvc-aircraft-2013b/data/images_family_val.txt'\n",
    "test_txt_path = f'{root}/fgvc-aircraft-2013b/data/images_family_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reorganize_dataset_txt(dataset_path, train_txt_path, new_dataset_path, 'train')\n",
    "reorganize_dataset_txt(dataset_path, val_txt_path, new_dataset_path, 'val')\n",
    "reorganize_dataset_txt(dataset_path, test_txt_path, new_dataset_path, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted file: /home/disi/ml/datasets/fgvc-aircraft-2013b.tar.gz\n",
      "Skipped folder: /home/disi/ml/datasets/val\n",
      "Skipped folder: /home/disi/ml/datasets/train\n",
      "Deleted folder: /home/disi/ml/datasets/fgvc-aircraft-2013b\n",
      "Skipped folder: /home/disi/ml/datasets/test\n"
     ]
    }
   ],
   "source": [
    "# @edit\n",
    "cleanup(f'{root}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flowers102 / Oxford Flowers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/102flowers.tgz to /home/disi/ml/datasets/flowers-102/102flowers.tgz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 344862509/344862509 [00:01<00:00, 260280841.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/disi/ml/datasets/flowers-102/102flowers.tgz to /home/disi/ml/datasets/flowers-102\n",
      "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/imagelabels.mat to /home/disi/ml/datasets/flowers-102/imagelabels.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 502/502 [00:00<00:00, 488071.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/setid.mat to /home/disi/ml/datasets/flowers-102/setid.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14989/14989 [00:00<00:00, 21089709.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# @edit\n",
    "root = \"/home/disi/ml/datasets/\"\n",
    "dataset = torchvision.datasets.Flowers102(root=root, download = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to your original dataset, .mat file, and new dataset location\n",
    "# @edit\n",
    "dataset_path = f'{root}/flowers-102/jpg'\n",
    "mat_path = f'{root}/flowers-102/imagelabels.mat'\n",
    "new_dataset_path = f'{root}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function for each of your train, validation, and test sets\n",
    "reorganize_dataset_mat(dataset_path, mat_path, new_dataset_path, 'train')\n",
    "reorganize_dataset_mat(dataset_path, mat_path, new_dataset_path, 'val')\n",
    "reorganize_dataset_mat(dataset_path, mat_path, new_dataset_path, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped folder: /home/disi/ml/datasets/val\n",
      "Deleted folder: /home/disi/ml/datasets/flowers-102\n",
      "Skipped folder: /home/disi/ml/datasets/train\n",
      "Skipped folder: /home/disi/ml/datasets/test\n"
     ]
    }
   ],
   "source": [
    "# @edit\n",
    "cleanup(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last step\n",
    "Once you reach the train-test-val configuration, run this final function to create a folder named as you wish that will contain train-test-val. This will be our root in the main.py. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @edi\n",
    "location = root\n",
    "newfolder_name = 'Flowers102'\n",
    "train_folder = f'{location}/train/'\n",
    "val_folder = f'{location}/val/'\n",
    "test_folder = f'{location}/test/'\n",
    "final_structure(location, newfolder_name, train_folder, val_folder, test_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_root = '/home/lorenzo/Desktop/qui/intromlproject_su cui lavorare/datasets/Flowers102'\n",
    "path = os.path.dirname(img_root) + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = os.path.basename(img_root)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the root path\n",
    "img_root = '/home/disi/ml/intromlproject/datasets/Aerei'\n",
    "\n",
    "# Standardize the path\n",
    "standardized_path = os.path.normpath(img_root)\n",
    "\n",
    "# Split the path into directory and the last component\n",
    "path, folder = os.path.split(standardized_path)\n",
    "\n",
    "# Print the results\n",
    "print(\"Path:\", path)\n",
    "print(\"Folder:\", folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
